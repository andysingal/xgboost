{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+ohrLWgfuQrBIl9kN8Sgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andysingal/xgboost/blob/main/xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost was specifically designed for speed. Speed gains allow machine learning models to build more quickly which is especially important when dealing with millions, billions, or trillions of rows of data. \n",
        "\n",
        "The following new design features give XGBoost a big edge in speed over comparable ensemble algorithms:\n",
        "\n",
        "- Approximate split-finding algorithm\n",
        "\n",
        "- Sparsity aware split-finding\n",
        "\n",
        "- Parallel computing\n",
        "\n",
        "- Cache-aware access\n",
        "\n",
        "- Block compression and sharding\n",
        "\n",
        "\n",
        "- Approximate split-finding algorithm:\n",
        "\n",
        "Decision trees need optimal splits to produce optimal results. A greedy algorithm selects the best split at each step and does not backtrack to look at previous branches.\n",
        "\n",
        "The split-finding algorithm uses quantiles, percentages that split data, to propose candidate splits. In a global proposal, the same quantiles are used throughout the entire training, and in a local proposal, new quantiles are provided for each round of splitting.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "- Sparsity-aware split finding\n",
        "\n",
        "Sparse data occurs when the majority of entries are 0 or null. A sparsity-aware split indicates that when looking for splits, XGBoost is faster because its matrices are sparse.\n",
        "\n",
        "According to the original paper, XGBoost: A Scalable Tree Boosting System, the sparsity-aware split-finding algorithm performed 50 times faster than the standard approach on the All-State-10K dataset.\n",
        "\n",
        "\n",
        "- Parallel computing\n",
        "Boosting is not ideal for parallel computing since each tree depends on the results of the previous tree. \n",
        "\n",
        "Parallel computing occurs when multiple computational units are working together on the same problem at the same time. XGBoost sorts and compresses the data into blocks. These blocks may be distributed to multiple machines, or to external memory.\n",
        "\n",
        "Sorting the data is faster with blocks. The split-finding algorithm takes advantage of blocks and the search for quantiles is faster due to blocks. In each of these cases, XGBoost provides parallel computing to expedite the model-building process.\n",
        "\n",
        "- Cache-aware access\n",
        "The data on your computer is separated into cache and main memory. The cache, what you use most often, is reserved for high-speed memory. The data that you use less often is held back for lower-speed memory. Different cache levels have different orders of magnitude of latency, as outlined here: https://gist.github.com/jboner/2841832.\n",
        "\n",
        "When it comes to gradient statistics, XGBoost uses cache-aware prefetching. XGBoost allocates an internal buffer, fetches the gradient statistics, and performs accumulation with mini batches. According to XGBoost: A Scalable Tree Boosting System, prefetching lengthens read/write dependency and reduces runtimes by approximately 50% for datasets with a large number of rows.\n",
        "\n",
        "- Block compression and sharding\n",
        "XGBoost delivers additional speed gains through block compression and block sharding.\n",
        "\n",
        "Block compression helps with computationally expensive disk reading by compressing columns. Block sharding decreases read times by sharding the data into multiple disks that alternate when reading the data.\n",
        "\n",
        "- Accuracy gains\n",
        "XGBoost adds built-in regularization to achieve accuracy gains beyond gradient boosting. Regularization is the process of adding information to reduce variance and prevent overfitting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SDBw3Ni4Ym-j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MpQ_7lEhW-Z-"
      },
      "outputs": [],
      "source": [
        "# File system manangement\n",
        "import time, psutil, os\n",
        "\n",
        "# Mathematical functions\n",
        "import math\n",
        "\n",
        "# Data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d.axes3d import get_test_data\n",
        "\n",
        "import seaborn as sns\n",
        "sns.set_theme()\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Runtime and memory usage**"
      ],
      "metadata": {
        "id": "0dlrK7KgcUTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recording the starting time, complemented with a stopping time check in the end to compute process runtime\n",
        "start = time.time()\n",
        "\n",
        "# Class representing the OS process and having memory_info() method to compute process memory usage\n",
        "process = psutil.Process(os.getpid())"
      ],
      "metadata": {
        "id": "7XAM1twpcJdK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZlT-97LGcbpY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}