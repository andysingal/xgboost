{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2hcDmPVSKYv7cEgBubRHQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andysingal/xgboost/blob/main/xgboost_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since boosted trees sum the predictions of previous trees, in addition to the prediction of the new tree . This is the idea behind additive training.\n",
        "\n",
        "- https://stats.stackexchange.com/questions/202858/xgboost-loss-function-approximation-with-taylor-expansion\n",
        "\n",
        "- IRIS DATASET\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bzmcm9ER9OjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8yWHNbtQ8wy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbe03910-d40c-45bd-f9e1-0c943159964f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 0.9736842105263158\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import datasets\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "df = pd.DataFrame(data= np.c_[iris['data'], iris['target']],columns= iris['feature_names'] + ['target'])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris['data'], iris['target'], random_state=2)\n",
        "\n",
        "#@XGBoost classification template\n",
        "xgb = XGBClassifier(booster='gbtree', objective='multi:softprob', \n",
        "                    learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)\n",
        "xgb.fit(X_train, y_train)\n",
        "y_pred = xgb.predict(X_test)\n",
        "score = accuracy_score(y_pred, y_test)\n",
        "print('Score: ' + str(score))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- a) booster='gbtree': The booster is the base learner. It's the machine learning model that is constructed during every round of boosting. You may have guessed that 'gbtree' stands for gradient boosted tree, the XGBoost default base learner. It's uncommon but possible to work with other base learners, a strategy we employ in Chapter 8, XGBoost Alternative Base Learners.\n",
        "\n",
        "- b) objective='multi:softprob': Standard options for the objective can be viewed in the XGBoost official documentation, https://xgboost.readthedocs.io/en/latest/parameter.html, under Learning Task Parameters. The multi:softprob objective is a standard alternative to binary:logistic when the dataset includes multiple classes. It computes the probabilities of classification and chooses the highest one. If not explicitly stated, XGBoost will often find the right objective for you.\n",
        "\n",
        "- c) max_depth=6: The max_depth of a tree determines the number of branches each tree has. It's one of the most important hyperparameters in making balanced predictions. XGBoost uses a default of 6, unlike random forests, which don't provide a value unless explicitly programmed.\n",
        "\n",
        "- d) learning_rate=0.1: Within XGBoost, this hyperparameter is often referred to as eta. This hyperparameter limits the variance by reducing the weight of each tree to the given percentage.\n",
        "\n",
        "- e) n_estimators=100: Popular among ensemble methods, n_estimators is the number of boosted trees in the model. Increasing this number while decreasing learning_rate can lead to more robust results."
      ],
      "metadata": {
        "id": "qbPBd35rjLtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Diabetes dataset\n",
        "Now that you are becoming familiar with scikit-learn and XGBoost, you are developing the ability to build and score XGBoost models fairly quickly. In this section, an XGBoost regressor template is provided using cross_val_score with scikit-learn's Diabetes dataset."
      ],
      "metadata": {
        "id": "LwtgTq62jmAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "X,y = datasets.load_diabetes(return_X_y=True)\n",
        "\n",
        "xgb = XGBRegressor(booster='gbtree', objective='reg:squarederror', \n",
        "                    learning_rate=0.1, n_estimators=100, random_state=2, n_jobs=-1)\n",
        "\n",
        "scores = cross_val_score(xgb, X, y, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Take square root of the scores\n",
        "rmse = np.sqrt(-scores)\n",
        "\n",
        "# Display accuracy\n",
        "print('RMSE:', np.round(rmse, 3))\n",
        "\n",
        "# Display mean score\n",
        "print('RMSE mean: %0.3f' % (rmse.mean()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4dydI8siD9f",
        "outputId": "41d5f20e-b6a3-4316-d153-c0d56a8b628f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: [63.011 59.705 64.538 63.706 64.588]\n",
            "RMSE mean: 63.109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rkuLUO7tjLR9"
      }
    }
  ]
}